#!/usr/bin/env python3
"""
Download facebook/bart-large-cnn model for chat summarization
Saves model to artifacts/models/bart-large-cnn/ folder
"""

import logging
import os
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def download_bart_model():
    """Download the BART-Large-CNN model to artifacts/models folder"""
    
    model_name = "facebook/bart-large-cnn"
    
    # Create artifacts/models directory
    artifacts_dir = Path("artifacts/models/bart-large-cnn")
    artifacts_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info("ü§ñ BART-Large-CNN Model Downloader")
    logger.info("=" * 50)
    logger.info(f"üì• Downloading {model_name}...")
    logger.info("üìä Model size: ~1.6GB")
    logger.info(f"üìÅ Target directory: {artifacts_dir}")
    
    try:
        # Check if model already exists
        if (artifacts_dir / "config.json").exists() and (artifacts_dir / "pytorch_model.bin").exists():
            logger.info("‚úÖ Model already exists in artifacts/models/bart-large-cnn")
            logger.info("üîÑ Testing existing model...")
            
            # Load existing model
            tokenizer = AutoTokenizer.from_pretrained(str(artifacts_dir))
            model = AutoModelForSeq2SeqLM.from_pretrained(str(artifacts_dir))
            
        else:
            # Check GPU availability
            if torch.cuda.is_available():
                device = "cuda"
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory // (1024**3)
                logger.info(f"üöÄ GPU detected: {gpu_name} ({gpu_memory}GB)")
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = "mps"
                logger.info("üçé Apple Silicon MPS detected")
            else:
                device = "cpu"
                logger.info("üíª Using CPU")
            
            # Download tokenizer to artifacts folder
            logger.info("üì• Downloading tokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            tokenizer.save_pretrained(str(artifacts_dir))
            logger.info(f"‚úÖ Tokenizer saved to {artifacts_dir}")
            
            # Download model to artifacts folder
            logger.info("üì• Downloading model (this may take a few minutes)...")
            model = AutoModelForSeq2SeqLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if device != "cpu" else torch.float32,
                low_cpu_mem_usage=True
            )
            model.save_pretrained(str(artifacts_dir))
            logger.info(f"‚úÖ Model saved to {artifacts_dir}")
        
        # Test the model
        logger.info("üß™ Testing model...")
        test_text = "This is a test conversation between two users discussing various topics."
        
        # Tokenize
        inputs = tokenizer.encode(test_text, return_tensors="pt", max_length=1024, truncation=True)
        
        # Generate summary
        with torch.no_grad():
            summary_ids = model.generate(
                inputs,
                max_length=50,
                min_length=10,
                length_penalty=2.0,
                num_beams=4,
                early_stopping=True
            )
        
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        logger.info(f"üéØ Test summary: {summary}")
        
        logger.info("\n" + "=" * 50)
        logger.info("üéâ BART Model Download Complete!")
        logger.info("=" * 50)
        logger.info("‚úÖ Model: facebook/bart-large-cnn")
        logger.info("‚úÖ Size: ~1.6GB")
        logger.info(f"‚úÖ Location: {artifacts_dir}")
        logger.info("‚úÖ Ready for chat summarization")
        logger.info("\nüì± Next steps:")
        logger.info("   1. Use chat summary feature in your app")
        logger.info("   2. Model will load from artifacts/models folder")
        logger.info("   3. Enjoy fast AI-powered summaries!")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Download failed: {e}")
        logger.error("üí° Try running: pip install transformers torch accelerate")
        return False

if __name__ == "__main__":
    success = download_bart_model()
    exit(0 if success else 1)
